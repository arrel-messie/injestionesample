# Druid Kafka Ingestion avec Protobuf et S3

Ce projet fournit une solution compl√®te pour l'ingestion de donn√©es Kafka vers Apache Druid en utilisant des sch√©mas Protobuf stock√©s sur S3, avec un pipeline CI/CD GitLab automatis√©.

## üèóÔ∏è Architecture

```
Kafka Topic (Protobuf) 
    ‚Üì
Druid Superviseur 
    ‚Üì (lit le schema depuis)
S3 Bucket (descriptors .desc)
    ‚Üì (versionn√© via)
GitLab CI/CD Pipeline
```

## üìÅ Structure du projet

```
druid-kafka-ingestion/
‚îú‚îÄ‚îÄ .gitlab-ci.yml              # Pipeline CI/CD
‚îú‚îÄ‚îÄ schemas/
‚îÇ   ‚îî‚îÄ‚îÄ proto/
‚îÇ       ‚îî‚îÄ‚îÄ settlement_transaction.proto  # Sch√©ma Protobuf source
‚îú‚îÄ‚îÄ druid-specs/
‚îÇ   ‚îî‚îÄ‚îÄ templates/
‚îÇ       ‚îî‚îÄ‚îÄ kafka-supervisor.json         # Template avec envsubst
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ dimensions.json          # D√©finition des dimensions Druid (JSON)
‚îÇ   ‚îú‚îÄ‚îÄ dev.env                  # Variables d'environnement dev
‚îÇ   ‚îú‚îÄ‚îÄ staging.env              # Variables d'environnement staging
‚îÇ   ‚îî‚îÄ‚îÄ prod.env                 # Variables d'environnement prod
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ compile-proto.sh         # Script de compilation des .proto
‚îÇ   ‚îú‚îÄ‚îÄ deploy-supervisor.sh     # Script de d√©ploiement
‚îÇ   ‚îî‚îÄ‚îÄ rollback-schema.sh       # Script de rollback
‚îî‚îÄ‚îÄ docs/
    ‚îú‚îÄ‚îÄ SETUP.md                 # Guide d'installation
    ‚îî‚îÄ‚îÄ DEPLOYMENT.md            # Guide de d√©ploiement
```

## ‚ú® Points forts de cette solution

‚úÖ **Zero d√©pendance externe** - Utilise uniquement des outils natifs Linux
- `envsubst` (pr√©-install√©, package gettext-base)
- `jq` (standard DevOps, ~3MB)
- `curl` (d√©j√† pr√©sent partout)

‚úÖ **Standard de l'industrie** - Approche utilis√©e par Kubernetes, Docker, Nginx

‚úÖ **Simple et maintenable** - Pas de "magie", syntaxe claire

‚úÖ **Performant** - Tr√®s rapide, images Docker l√©g√®res

## üöÄ D√©marrage rapide

### Pr√©requis

- Acc√®s GitLab avec CI/CD activ√©
- Bucket S3 configur√© (ex: `my-company-druid-schemas`)
- Credentials AWS configur√©s dans GitLab CI/CD
- Cluster Druid avec acc√®s S3
- Cluster Kafka avec authentification SASL_SSL

### Configuration initiale

1. **Configurer les variables GitLab CI/CD**
   
   Dans `Settings > CI/CD > Variables`, ajouter :
   - `AWS_ACCESS_KEY_ID` - Acc√®s S3
   - `AWS_SECRET_ACCESS_KEY` - Secret S3
   - `KAFKA_PROD_USER` - Username Kafka production
   - `KAFKA_PROD_PASSWORD` - Password Kafka production
   - `S3_BUCKET` - Nom du bucket (ex: `my-company-druid-schemas`)
   - `S3_REGION` - R√©gion AWS (ex: `eu-west-1`)

2. **Adapter les fichiers de configuration**
   
   Modifier les fichiers dans `config/` selon vos environnements

3. **D√©finir votre sch√©ma Protobuf**
   
   √âditer `schemas/proto/settlement_transaction.proto`

4. **D√©finir vos dimensions Druid**
   
   √âditer `config/dimensions.json`

### D√©ploiement

1. **Push vers develop** ‚Üí D√©ploie automatiquement en DEV
2. **Push vers staging** ‚Üí D√©ploie automatiquement en STAGING
3. **Push vers main/master** ‚Üí D√©ploie manuellement en PROD

## üîÑ Template envsubst

Le projet utilise `envsubst` pour la substitution de variables :

```json
{
  "topic": "${KAFKA_TOPIC}",
  "taskCount": ${TASK_COUNT:-10}
}
```

**Syntaxe :**
- `${VAR}` - Variable obligatoire
- `${VAR:-default}` - Variable avec valeur par d√©faut

## üìä Versioning des sch√©mas

Chaque commit g√©n√®re une version de sch√©ma :
- `s3://bucket/schemas/{COMMIT_SHA}/` - Version sp√©cifique
- `s3://bucket/schemas/develop-latest/` - Derni√®re version develop
- `s3://bucket/schemas/stable/` - Version stable (main/master)

## üîß Configuration Druid

### Extensions requises
```properties
druid.extensions.loadList=["druid-s3-extensions", "druid-protobuf-extensions", "druid-kafka-indexing-service"]
```

### Configuration S3
```properties
druid.storage.type=s3
druid.storage.bucket=my-company-druid-segments
druid.s3.accessKey=${AWS_ACCESS_KEY_ID}
druid.s3.secretKey=${AWS_SECRET_ACCESS_KEY}
```

## üõ†Ô∏è Commandes utiles

### D√©ploiement
```bash
make deploy-dev      # D√©ployer en DEV
make deploy-staging  # D√©ployer en STAGING
make deploy-prod     # D√©ployer en PRODUCTION
```

### Validation
```bash
make validate        # Valider la configuration
make compile         # Compiler les .proto
```

### Monitoring
```bash
make status ENV=dev  # Statut du superviseur
make logs ENV=dev    # Logs du superviseur
```

### Rollback
```bash
make rollback ENV=prod VERSION=abc123f
```

### Compilation manuelle
```bash
./scripts/compile-proto.sh
```

### Test local
```bash
source config/dev.env
export DIMENSIONS_JSON=$(cat config/dimensions.json | jq -c .)
envsubst < druid-specs/templates/kafka-supervisor.json > test-output.json
jq . test-output.json  # V√©rifier le JSON
```

## üìñ Documentation

- [Guide d'installation d√©taill√©](docs/SETUP.md)
- [Guide de d√©ploiement](docs/DEPLOYMENT.md)
- [D√©marrage rapide 5 minutes](QUICKSTART.md)

## üîí S√©curit√©

- Les credentials sont stock√©s dans GitLab CI/CD Variables (masqu√©s)
- Descriptors S3 accessibles en lecture seule par Druid
- SASL_SSL activ√© pour Kafka

## üêõ Troubleshooting

### Le superviseur ne d√©marre pas
1. V√©rifier le descriptor sur S3
2. V√©rifier les permissions IAM
3. Consulter les logs Druid

### Erreurs de parsing Protobuf
1. V√©rifier `protoMessageType`
2. V√©rifier compilation avec `--include_imports`

### JSON invalide
```bash
jq empty supervisor-spec.json  # Valider
```

## üìù License

Propri√©taire - Usage interne uniquement

## üë• Contributeurs

Votre √©quipe Data Engineering
