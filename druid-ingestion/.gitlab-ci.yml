stages:
  - build
  - deploy

variables:
  PROTO_MESSAGE_TYPE: "com.company.PaymentTransactionEvent"
  PROTO_FILE: "schemas/proto/settlement_transaction.proto"
  PROTO_DESC_OUTPUT: "schemas/compiled/settlement_transaction.desc"

# Build stage: Compile proto and generate spec
build:
  stage: build
  image: alpine:latest
  before_script:
    - apk add --no-cache bash curl jq yq python3 py3-pip protobuf protobuf-dev
    - pip3 install pyyaml
    - chmod +x druid-ingestion.sh
  script:
    # Compile protobuf descriptor
    - mkdir -p schemas/compiled
    - protoc --descriptor_set_out="${PROTO_DESC_OUTPUT}" --proto_path=schemas/proto "${PROTO_FILE}"
    - |
      echo "âœ… Protobuf descriptor compiled: ${PROTO_DESC_OUTPUT}"
      ls -lh "${PROTO_DESC_OUTPUT}"
    
    # Generate supervisor spec for all environments
    - |
      for env in dev staging prod; do
        echo "ðŸ“¦ Generating spec for environment: $env"
        ./druid-ingestion.sh build -e "$env" || echo "âš ï¸ Failed to generate spec for $env"
      done
  artifacts:
    paths:
      - schemas/compiled/*.desc
      - druid-specs/generated/*.json
    expire_in: 1 week
  only:
    - main
    - develop
    - merge_requests

# Deploy to DEV
deploy:dev:
  stage: deploy
  image: alpine:latest
  environment:
    name: dev
    url: ${DRUID_OVERLORD_URL_DEV}
  before_script:
    - apk add --no-cache bash curl jq yq python3 py3-pip aws-cli
    - pip3 install pyyaml
    - chmod +x druid-ingestion.sh
    # Load environment-specific config
    - |
      cat > config/dev.env << EOF
      KAFKA_BOOTSTRAP_SERVERS="${KAFKA_BOOTSTRAP_SERVERS_DEV}"
      KAFKA_SECURITY_PROTOCOL="${KAFKA_SECURITY_PROTOCOL_DEV}"
      KAFKA_SASL_MECHANISM="${KAFKA_SASL_MECHANISM_DEV}"
      KAFKA_SASL_JAAS_CONFIG="${KAFKA_SASL_JAAS_CONFIG_DEV}"
      KAFKA_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM="${KAFKA_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_DEV:-}"
      KAFKA_TOPIC="${KAFKA_TOPIC_DEV}"
      DRUID_URL="${DRUID_OVERLORD_URL_DEV}"
      DATASOURCE="${DATASOURCE_NAME_DEV}"
      PROTO_DESCRIPTOR_PATH="${PROTO_DESCRIPTOR_PATH_DEV}"
      EOF
  script:
    # Upload descriptor to S3
    - |
      if [ -n "${S3_BUCKET_DEV}" ]; then
        echo "ðŸ“¤ Uploading descriptor to S3: s3://${S3_BUCKET_DEV}/schemas/settlement_transaction.desc"
        aws s3 cp "${PROTO_DESC_OUTPUT}" "s3://${S3_BUCKET_DEV}/schemas/settlement_transaction.desc" \
          --region "${S3_REGION_DEV:-us-east-1}" || exit 1
        echo "âœ… Descriptor uploaded successfully"
      else
        echo "âš ï¸ S3_BUCKET_DEV not set, skipping S3 upload"
      fi
    
    # Deploy supervisor spec
    - echo "ðŸš€ Deploying supervisor to Druid..."
    - ./druid-ingestion.sh deploy -e dev
    - echo "âœ… Deployment completed"
  only:
    - main
    - develop
  when: manual
  allow_failure: false

# Deploy to STAGING
deploy:staging:
  stage: deploy
  image: alpine:latest
  environment:
    name: staging
    url: ${DRUID_OVERLORD_URL_STAGING}
  before_script:
    - apk add --no-cache bash curl jq yq python3 py3-pip aws-cli
    - pip3 install pyyaml
    - chmod +x druid-ingestion.sh
    - |
      cat > config/staging.env << EOF
      KAFKA_BOOTSTRAP_SERVERS="${KAFKA_BOOTSTRAP_SERVERS_STAGING}"
      KAFKA_SECURITY_PROTOCOL="${KAFKA_SECURITY_PROTOCOL_STAGING}"
      KAFKA_SASL_MECHANISM="${KAFKA_SASL_MECHANISM_STAGING}"
      KAFKA_SASL_JAAS_CONFIG="${KAFKA_SASL_JAAS_CONFIG_STAGING}"
      KAFKA_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM="${KAFKA_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_STAGING:-}"
      KAFKA_TOPIC="${KAFKA_TOPIC_STAGING}"
      DRUID_URL="${DRUID_OVERLORD_URL_STAGING}"
      DATASOURCE="${DATASOURCE_NAME_STAGING}"
      PROTO_DESCRIPTOR_PATH="${PROTO_DESCRIPTOR_PATH_STAGING}"
      EOF
  script:
    - |
      if [ -n "${S3_BUCKET_STAGING}" ]; then
        echo "ðŸ“¤ Uploading descriptor to S3: s3://${S3_BUCKET_STAGING}/schemas/settlement_transaction.desc"
        aws s3 cp "${PROTO_DESC_OUTPUT}" "s3://${S3_BUCKET_STAGING}/schemas/settlement_transaction.desc" \
          --region "${S3_REGION_STAGING:-us-east-1}" || exit 1
        echo "âœ… Descriptor uploaded successfully"
      fi
    - ./druid-ingestion.sh deploy -e staging
  only:
    - main
  when: manual
  allow_failure: false

# Deploy to PROD
deploy:prod:
  stage: deploy
  image: alpine:latest
  environment:
    name: production
    url: ${DRUID_OVERLORD_URL_PROD}
  before_script:
    - apk add --no-cache bash curl jq yq python3 py3-pip aws-cli
    - pip3 install pyyaml
    - chmod +x druid-ingestion.sh
    - |
      cat > config/prod.env << EOF
      KAFKA_BOOTSTRAP_SERVERS="${KAFKA_BOOTSTRAP_SERVERS_PROD}"
      KAFKA_SECURITY_PROTOCOL="${KAFKA_SECURITY_PROTOCOL_PROD}"
      KAFKA_SASL_MECHANISM="${KAFKA_SASL_MECHANISM_PROD}"
      KAFKA_SASL_JAAS_CONFIG="${KAFKA_SASL_JAAS_CONFIG_PROD}"
      KAFKA_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM="${KAFKA_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_PROD:-}"
      KAFKA_TOPIC="${KAFKA_TOPIC_PROD}"
      DRUID_URL="${DRUID_OVERLORD_URL_PROD}"
      DATASOURCE="${DATASOURCE_NAME_PROD}"
      PROTO_DESCRIPTOR_PATH="${PROTO_DESCRIPTOR_PATH_PROD}"
      EOF
  script:
    - |
      if [ -n "${S3_BUCKET_PROD}" ]; then
        echo "ðŸ“¤ Uploading descriptor to S3: s3://${S3_BUCKET_PROD}/schemas/settlement_transaction.desc"
        aws s3 cp "${PROTO_DESC_OUTPUT}" "s3://${S3_BUCKET_PROD}/schemas/settlement_transaction.desc" \
          --region "${S3_REGION_PROD:-us-east-1}" || exit 1
        echo "âœ… Descriptor uploaded successfully"
      fi
    - ./druid-ingestion.sh deploy -e prod
  only:
    - main
  when: manual
  allow_failure: false

